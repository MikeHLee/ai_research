1

# Self-Supervised Learning for Large-Scale Preventive Security Constrained DC Optimal Power Flow


Seonho Park and Pascal Van Hentenryck



_**Abstract**_ **—Security-Constrained Optimal Power Flow (SCOPF)**
**plays a crucial role in power grid stability but becomes in-**
**creasingly complex as systems grow. This paper introduces**
**Primal-Dual Learning (PDL) for SCOPF (PDL-SCOPF), a self-**
**supervised end-to-end primal-dual learning framework for pro-**
**ducing near-optimal solutions to large-scale SCOPF problems**
**in milliseconds. Indeed, PDL-SCOPF remedies the limitations**
**of supervised counterparts that rely on training instances with**
**their optimal solutions, which becomes impractical for large-**
**scale SCOPF problems. PDL-SCOPF mimics an Augmented La-**
**grangian Method (ALM) for training primal and dual networks**
**that learn the primal solutions and the Lagrangian multipliers,**
**respectively, to the unconstrained optimizations. In addition,**
**PDL-SCOPF incorporates a repair layer to ensure the feasibility**
**of the power balance in the nominal case, and a binary search**
**layer to compute, using the Automatic Primary Response (APR),**
**the generator dispatches in the contingencies. The resulting**
**differentiable program can then be trained end-to-end using**
**the objective function of the SCOPF and the power balance**
**constraints of the contingencies. Experimental results demon-**
**strate that the PDL-SCOPF delivers accurate feasible solutions**
**with minimal optimality gaps. The framework underlying PDL-**
**SCOPF aims at bridging the gap between traditional optimization**
**methods and machine learning, highlighting the potential of**
**self-supervised end-to-end primal-dual learning for large-scale**
**optimization tasks.**


_**Index Terms**_ **—Primal-dual learning, Security-constrained op-**
**timal power flow, Column and constraint generation, Deep**
**learning, Differentiable programming, End-to-end learning**


N OMENCLATURE

The next list describes several symbols that will be later
used. Sets and indices are in calligraphic or blackboard bold
font. Bold symbols represent vectors or matrices.
**Machine learning related**
_**λ**_ Dual variable estimate associated with the power
balance constraint under the generator contingencies
_D_ _ϕ_ Dual network
**g** ˜ _k_ Dispatch estimate under the generator contingency _k_
_P_ _θ_ Primal network
_**η**_ ˜ Slack variable estimate
**x** Input parameters
**g** ˜ Base case dispatch estimate
**g** ˇ Base case dispatch estimate before applying the power
balance repair layer
**Parameters**

**B** Generator–bus incidence matrix

_β_ Threshold parameter for choosing the cuts to be added
in the CCGA


The authors are affiliated with the School of Industrial and Systems
Engineering, Georgia Institute of Technology, Atlanta, GA 30332, USA, Email: seonho.park@gatech.edu, pascal.vanhentenryck@isye.gatech.edu



_δ_ Relaxation parameter for the binary search layer
**f** _,_ **f** Lower and upper bound of power flow limit
_**γ**_ Primary response parameter
**g** ˆ Generator capacities
**g** _,_ ~~**g**~~ Lower and upper bound of generator dispatch
**d** Load demands

**L** Line outage distribution factor matrix
**c** Cost coefficients
**K** Power transfer distribution factor matrix

_M_ _η_ Penalty coefficient
**Set and indices**
_G, E, N_ _, L_ Indices of the generators, transmission lines, buses,
load units
_K_ _g_ _, K_ _e_ Generator or transmission line contingency indices
K _g_ Subset of _K_ _g_ considered in the current CCGA iteration
U _g_ _,_ U _e_ Sets of pairs of ( _k, l_ ) where the power flow limit
constraint at line _l_ is violated under the generator or
line contingency _k_
**Variables**

**f** Base case power flow vector
**f** _k_ Power flow vector under the generator contingency _k_
**g** Power generations (dispatches, injections)
**g** _k_ Power generations under the generator contingency _k_
**g** _k_ _[′]_ Provisional variables for **g** _k_
_n_ _k_ Global signal variable under the generator contingency
_k_

_**ρ**_ _k_ Binary variable indicating whether **g** _k_ reaches the
upper limit under the generator contingency _k_
_**η**_ Slack variable for power flow limit constraints


I. I NTRODUCTION


Power system operations must maintain the power balance
between load and generation at all times. To achieve this
equilibrium requires solving mathematical optimization problems in real-time and day-ahead electricity markets. These
optimizations have become increasingly challenging in recent
years due to the integration of renewable energy sources which
are both more numerous and volatile compared to traditional
synchronous generators. As a consequence, managing the reliability and the risk in the system gets increasingly important.
The Security-Constrained Optimal Power Flow (SCOPF)
problem is a traditional model to ensure the stability of
power systems operations under contingencies. The SCOPF
guarantees that a feasible generator dispatch exists even under the failure of a single generator or transmission line,
while minimizing the cost of the nominal dispatch. In actual
operations, however, Independent System Operators (ISOs)


do not solve the SCOPF; rather they maintain reserves that
are computed outside the markets; the reserve requirements
may be sub-optimal or insufficient in cases with large shares
of renewables. One reason for using reserves is the high
computational cost of the SCOPF compared to its equivalent
Economic Dispatch (ED) with reserves, even when using
a DC approximation of the power flow equations. Indeed,
the DC-SCOPF is a Mixed-Integer Linear Program (MILP)
whose number of binary variables increases quadratically
with the number of contingencies for preventive SCOPF with
Automatic Primary generator Response (APR) [1]. In contrast,
the ED with the reserve constraints is a linear program that
can be readily solved.
One possible approach to address the computational challenges of the SCOPF is the use of Machine Learning (ML)
which has attracted significant attention in the power systems
community in recent years. However, even for machine learning, the SCOPF raises fundamental challenges since generating
sufficient training data is typically impractical for industrialsize power grids. Moreover, the ML models may have to
predict a quadratic number of variables to ensure the power
balance constraints in the contingencies.
This paper takes a different avenue. It considers the ED
formulation used by the ISOs in the United States where the
reserve requirements have been replaced by _N−_ 1 contingencies
to produce a preventive APR-based SCOPF. The paper then
proposes an _end-to-end self-supervised primal-dual Learning_
_framework_, called PDL-SCOPF, to address the computational
challenges raised by the SCOPF formulation. By virtue of
being self-supervised, PDL-SCOPF does not need the optimal
solutions of thousands of training instances: it just relies on
the availability of an empirical distribution of the input configuration that captures future conditions of interest. Thanks to
being trained end-to-end, PDL-SCOPF only needs to predict
the baseline dispatch: differentiable layers are then used to
restore the feasibility of the power balance for the base case
and to predict the contingency dispatches using the APR. PDLSCOPF employs a Primal-Dual Learning (PDL) framework
whose training mimics the Augmented Lagrangian Method
(ALM) over a set of instances: it uses a primal network to
approximate unconstrained optimization problems similar to
those of the ALM and a dual network to approximate the
Lagrangian multipliers used in these optimizations.
The main contribution of the paper, PDL-SCOPF, can be
summarized as follows:


_•_ PDL-SCOPF is a self-supervised method that produces
near-optimal feasible solutions to preventive DC-SCOPF.
Thus, it does not require a training dataset that contains
optimal SCOPF solutions for a set of instances.

_•_ PDL-SCOPF is a Primal-Dual Learning framework that
mimics Augmented Lagrangian Method by predicting
both primal optimal solutions (using a primal network)
and their associated Lagrangian multiplier (using a dual
network).

_•_ PDL-SCOPF uses differential layers to restore the feasibility of the power balance [2] and to adapt the binary
search from the Column and Constraint Generation Al
gorithm in [3] to compute the contingency dispatches



2


and their violations. The predictive model and these
differentiable layers can then be trained end-to-end in the
framework to produce near-optimal feasible solutions.

_•_ The scalability and accuracy of PDL-SCOPF have been
validated on industry-size test cases with thousands of
buses, for which it estimates solutions in milliseconds.

The rest of the paper is structured as follows. Section II
describes the related work in SCOPF and machine learning.
Section III revisits the formulation of SCOPF and briefly
describes a Column and Constraint Generation Algorithm
to solve it exactly. Section IV revisits the PDL framework
and Section V introduces PDL-SCOPF. Section VI presents
the experimental results and compares PDL-SCOPF with a
number of baselines. Section VII summarizes the paper and
presents directions for future work.


II. R ELATED W ORK


_a) Security-Constrained OPF:_ This paper focuses on the
SCOPF problem with _N_ _−_ 1 generator and line contingencies.
A comprehensive review of SCOPF can be found in [4]. The
SCOPF aims at determining the pre-contingency generator
dispatch that minimizes operational costs while ensuring feasibility even in the event of contingencies. A _N−_ 1 contingency
refers to the outage of any single component (generator or
line). The SCOPF can be classified into two distinct settings:
1) the corrective case, discussed in [5], which assumes that
re-scheduling is possible, and 2) the preventive case, studied
for instance in [1], [3], [6], [7], where re-dispatch is not an
option. This paper considers a preventive SCOPF that models
an APR for generator contingencies. Such SCOPF problems
are computationally challenging due to the binary variables
introduced to model the APR response. Indeed, the number
of binary variables increases quadratically in the number
of generator contingencies. To alleviate this computational
burden, various decomposition techniques have been proposed:
they include Benders decomposition (e.g., [8], and the Column
and Constraint Generation Algorithm (CCGA) proposed in [3],

[9]. The PDL-SCOPF method proposed in this paper uses
some insights from the CCGA. Extensive reviews on ACSCOPF are available in [10], [11]. Recent efforts to tackle ACSCOPF, as part of the Grid Optimization Competition initiated
by ARPA-E, are summarized in [12]. Both industry and
academia have also explored approximations and relaxations to
the AC-SCOPF, as discussed in [13], [14]. Currently, industry
(e.g., ISO) predominantly employs the DC formulation for
SCOPF (DC-SCOPF) [15]. Consequently, this paper focuses
on the DC-SCOPF. More precisely, the formulation presented
in this paper can be viewed as the economic dispatch of the
ISOs where the reserve constraints have been replaced by
_N −_ 1 contingencies under the APR modeling. For a broader
perspective, see the recent comprehensive review on large grid
optimization in [16].
_b) Machine Learning for Optimization and Power Sys-_
_tems:_ Various end-to-end Machine Learning (ML) approaches
have recently emerged for optimization applications. They aim
at directly estimating optimal solutions given a distribution of
the input parameters. In other words, these ML approaches are


3


_̸_


_̸_


_̸_


_̸_


_̸_



used as regressions to approximate the mapping from input
parameters to optimal solutions. A comprehensive overview
of end-to-end optimization learning methods can be found in

[17]. A ML framework to accelerate general mixed integer
programming (MIP) solvers was proposed in [18]; it also offers
a way to bound the optimality gap between the inference
and the optimal solution. End-to-end optimization learning
techniques have also founded applications in various power
system optimization tasks, such as economic dispatch [2],
DCOPF [19], ACOPF [20], and unit commitment [21], [22].
_c) Supervised Learning for SCOPF:_ Several supervised _̸_
learning approaches have been applied to SCOPF. D EEP OPF [19] approximates the optimal dispatch of the SCOPF _̸_
problem, taking into account line contingencies. Velloso and _̸_
Van Hentenryck [23] combine a deep neural network-based _̸_
mapping with the CCGA method to approximate the optimal
solution of the SCOPF problem, with a focus on generator contingencies. These approaches employed supervised learning,
which requires gathering pairs of input parameters and their _̸_
corresponding optimal solutions for training. As demonstrated
in Section VI, solving a single SCOPF instance for large-scale
SCOPF is quite time-consuming, which makes the collection
of such training data impractical. This paper remedies this
limitation by using a self-supervised learning framework that
does not necessitate the ground truth solutions for training,
which, in turn, is especially effective for learning large-scale
industry-sized SCOPF problems having both generator and
line contingencies.
_d) Self-Supervised Learning for OPF:_ Self-supervised
learning leverages the original optimization formulation, including the objective function, to approximate optimal solutions without relying on the ground truth data. DC3 [24]
is such a self-supervised method that employs an implicit
layer to driving the learning towards feasible solutions. E2ELR

[2] introduces repair layers for satisfying power balance and
reserve constraints that are trained end-to-end to produce
feasible and near-optimal solutions to ED problems. PDL

[25] integrates primal and dual networks to approximate both
the primal and dual solutions for constrained optimization
problems. In this work, PDL-SCOPF leverages the E2ELR
repair layer for power balance, PDL, and the binary search
layer adapted from CCGA to produce near-optimal feasible
solutions to industry-size SCOPF problems.
_e) Scalable Learning for OPF:_ Significant work has
been carried out for scaling ML approach to large-scale
OPF. They include Compact Learning [26], which learns a
lower-rank representation of the optimal solution for ACOPF,
and Spatial Decomposition [27], which proposes a two-step
learning process by learning the flows between regions in a
spatial decomposition before learning the optimal power flow
in each region.


III. P ROBLEM F ORMULATION AND O BJECTIVE


This section reviews the SCOPF problem considered in this
paper. The formulation can be seen as the traditional economic
dispatch of the ISOs in the United States, where the reserve
constraints have been replaced by an explicit modeling of _N−_ 1



**Model 1** The Extensive SCOPF Formulation


_̸_


_̸_


_̸_


_̸_


_̸_




[ _**η**_ _k_ ] _k∈{_ 0 _}∪Kg_ _∪Ke_

**s. t.** : **1** _[⊤]_ **g** = **1** _[⊤]_ **d** (2)


**f** _−_ _**η**_ 0 _≤_ **f** = **K** ( **d** _−_ **Bg** ) _≤_ **f** + _**η**_ 0 (3)


**g** _≤_ **g** _≤_ ~~**g**~~ (4)


**1** _[⊤]_ **g** _k_ = **1** _[⊤]_ **d** _∀k ∈K_ _g_ (5)


**f** _−_ _**η**_ _k_ _≤_ **f** _k_ = **K** ( **d** _−_ **Bg** _k_ ) _≤_ **f** + _**η**_ _k_ _∀k ∈K_ _g_ (6)

_g_ ~~_i_~~ _≤_ _g_ _k,i_ _≤_ ~~_g_~~ _i_ _∀i_ _∈G, ∀k ∈K_ _g_ _, i_ = _̸_ _k_ (7)


_g_ _k,k_ = 0 _∀k ∈K_ _g_ (8)


_|g_ _k,i_ _−g_ _i_ _−n_ _k_ _γ_ _i_ _g_ ˆ _i_ _|≤_ _g_ ˆ _i_ _ρ_ _k,i_ _∀i_ _∈G, ∀k ∈K_ _g_ _, i_ = _̸_ _k_ (9)

_g_ _i_ + _n_ _k_ _γ_ _i_ _g_ ˆ _i_ _≥_ _g_ ˆ _i_ _ρ_ _k,i_ + _g_ ~~_i_~~ _∀i_ _∈G, ∀k ∈K_ _g_ _, i_ = _̸_ _k_ (10)

_g_ _k,i_ _≥_ _g_ ˆ _i_ _ρ_ _k,i_ + _g_ ~~_i_~~ _∀i_ _∈G, ∀k ∈K_ _g_ _, i_ = _̸_ _k_ (11)


**f** _−_ _**η**_ _k_ _≤_ **f** + _f_ _k_ **L** _k_ _≤_ **f** + _**η**_ _k_ _∀k ∈K_ _e_ (12)


_**η**_ _k_ _≥_ 0 _∀k ∈{_ 0 _}∪K_ _g_ _∪K_ _e_ (13)


_n_ _k_ _∈_ [0 _,_ 1] _∀k ∈K_ _g_ (14)


_ρ_ _k,i_ _∈{_ 0 _,_ 1 _}_ _∀i_ _∈G, ∀k ∈K_ _g_ _, i_ = _̸_ _k_ (15)


contingencies. In the formulation, the thermal limits are thus
considered as soft constraints but their violations are heavily
penalized in the objective function. To handle contingencies,
the formulation captures the Automatic Primary Response
(APR) [1], [12] used in [3], [23].


_A. The SCOPF Problem_


Model 1 presents the extensive SCOPF problems with _N −_ 1
generator and line contingencies. Its primary objective is to
determine the optimal active generation points for the base
case while ensuring the feasibility of both generator and line
contingencies. Objective (1) sums the linear cost of the base
case dispatch **g** and the penalties for violating the thermal
limits in the base case and in the contingencies, with _M_ _η_
being a large penalty coefficient.
_a) The Base Case:_ The base case includes Constraints (2), (3), and (4) that should be satisfied. Constraint (2)
imposes the power balance between the generation and the
load demands in the nominal case. In this constraint, **1**
represents a vector whose elements are all one, and **d** _∈_ R _[|N |]_

is the load demand with the bus-wise representation of the load
units. Constraint (3) ensures that the power flow **f** for each
transmission line falls within the predefined upper and lower
limits, **f** and **f** . Any violation is penalized in the objective
value. The power flow for the base case **f** = **K** ( **d** _−_ **Bg** ) is
computed using a Power Transfer Distribution Factor (PTDF)
matrix **K** _∈_ R _[|E|×|N |]_ and a generator-bus incidence matrix **B** .
Constraint (4) imposes the generation limits.
_b) Generator Contingency:_ Each generator contingency
imposes Constraints (5), (6), and (7) to enforce the power
balance, the thermal limits, and the generation bounds under
the generator contingency. The only difference from the base
case is Constraint (8) that specifies that generator _k_ should
remain inactive under its contingency.
_c) Automatic Primary Response for Generator Contin-_
_gency:_ To address generator contingencies, Constraints (9)–



**c** _[⊤]_ **g** + _M_ _η_


_̸_


_̸_


_̸_


_̸_


_̸_



� _∥_ _**η**_ _k_ _∥_ 1

_k∈{_ 0 _}∪Kg_ _∪Ke_


_̸_


_̸_


_̸_


_̸_


_̸_






_̸_


_̸_


_̸_


_̸_


_̸_



 (1)


_̸_


_̸_


_̸_


_̸_


_̸_



**min** **c**

**g** _,_

[ **g** _k,_ _**ρ**_ _k,nk_ ] _k∈Kg,_


_̸_


_̸_


_̸_


_̸_


_̸_






_̸_


_̸_


_̸_


_̸_


_̸_



�
 _k∈{_ 0 _}∪K_


_̸_


_̸_


_̸_


_̸_


_̸_


4


_̸_


_̸_


_̸_


_̸_


_̸_



**Algorithm 1** Column-and-Constraint Generation Algorithm
(CCGA) for solving SCOPF


Initialize: K _g_ = _∅_, U _g_ = _∅_, U _e_ = _∅_

1: **for** _j_ = 0 _,_ 1 _, . . ._ **do**
2: Solve Model 2 and obtain **g** [(] _[j]_ [)]
3: Obtain **g** _k_ [(] _[j]_ [)] using the binary search, _∀k ∈K_ _g_
4: Compute the violations _α_ _[g]_ _k,l_ [w.r.t. (26),] _[ ∀][l]_ _[∈E][,][ ∀][k][ ∈K]_ _[g]_
5: Compute the violations _α_ _[e]_ _k,l_ [w.r.t. (27),] _[ ∀][l]_ _[∈E][,][ ∀][k][ ∈K]_ _[e]_
6: **Break** if max _{α_ _[g]_ _k,l_ _[} ≤]_ _[ϵ]_ [ and][ max] _[{][α]_ _k,l_ _[e]_ _[} ≤]_ _[ϵ]_
7: U _g_ _←{_ ( _k, l_ ) _| α_ _[g]_ _k,l_ _[>]_ [ max] _[{][α]_ _[g]_ _k,l_ _[}][/β][}]_
8: U _e_ _←{_ ( _k, l_ ) _| α_ _[e]_ _k,l_ _[>]_ [ max] _[{][α]_ _[e]_ _k,l_ _[}][/β][}]_
9: K _g_ _←_ K _g_ _∪{k | ∀k ∈_ U _g_ _}_ _̸_
10: **end for**


(11) implements an APR [1], [12]. The formulation in this _̸_
paper is based on the APR model used in [3], [23] where, for _̸_
each generator contingency _k_, a system-wide signal _n_ _k_ _∈_ [0 _,_ 1] _̸_
represents the level of system response required to resolve
the power imbalance. Furthermore, the APR model assumes
that the change in synchronized generation dispatch under
a contingency is proportionate to the droop slope, which is
determined by the product of generator capacity ˆ _g_ and the
predefined parameter _γ_ as in [3]. The generator capacities
are defined as ˆ **g** = ~~**g**~~ _−_ **g** . The APR constraints ensure that
the generation dispatch under generator contingency remains
within the generation limits. The mathematical expression for
constraining the generation dispatch for synchronized generators can be defined as:


ˆ
_g_ _k,i_ = min _{g_ _i_ + _n_ _k_ _γ_ _i_ _g_ _i_ _,_ ~~_g_~~ _i_ _}, ∀i_ _∈G, ∀k_ _∈K_ _g_ _, i_ = _̸_ _k._ (16)


To represent the disjunctive constraint (16), binary variables
_ρ_ _k,i_ are introduced. For all _i ∈G_ and _k ∈K_ _g_ such that _k ̸_ = _i_,
these binary variables enable the mathematical representation
of the above disjunctive constraint (16). The binary variable _ρ_
imposes _g_ _k_ = ~~_g_~~ if _ρ_ = 1, and _g_ _k_ = _g_ + _n_ _k_ _γg_ ˆ otherwise. As a
result, the extensive SCOPF problem 1, featuring the APR of
generators under generator contingencies, is a MILP problem.
_d) Line Contingency:_ Line contingencies impose Constraints (12). These constraints ensure that, during a line
contingency, there is an immediate redistribution of power flow
specified by the Line Outage Distribution Factor (LODF) [28],

[29]. Under the line contingency of _k_, _k_ -th column vector,
denoted as **L** _k_, of the LODF matrix **L** _∈_ R _[|E|×|K]_ _[e]_ _[|]_, delineates
the redistribution of base case power flow at line _k_, _f_ _k_, to the
other lines, so as to ensure that there is no power flow at line
_k_, i.e., _f_ _k,k_ = 0 _, ∀k ∈K_ _e_ .
_e) Slack Variables:_ The constraints related to thermal
limits (3, 6, and 12) are treated as _soft constraints_ . Using soft
constraints for the thermal limits is in accordance with the

formulations used by ISOs for clearing the electricity markets
in the United States [30], [31]. These soft constraints capture
positive slack variables, as defined in (13), which are heavily
penalized in the objective function.


_B. The Column and Constraint Generation Algorithm (CCGA)_


Solving the extensive SCOPF model 1 directly is extremely
challenging for large networks, as the number of binary
variables grows quadratically with respect to the number of
generators. The optimization algorithm, CCGA, was proposed



**Model 2** The CCGA Master Problem


_̸_


_̸_


_̸_


_̸_


_̸_



_η_ _k,l_

( _k,l_ ) _∈_ U _g_ _∪_ U _e_


_̸_


_̸_


_̸_


_̸_


_̸_



**c** _[⊤]_ **g** + _M_ _η_


_̸_


_̸_


_̸_


_̸_


_̸_



**min** **c**
**g** _,_ _**η**_ 0 _,_ [ **g** _k_ _[′]_ []] _k∈Kg_ _[,]_

[ _**ρ**_ _k,nk_ ] _k∈_ K _g,_
� _ηk,l_ �( _k,l_ ) _∈_ U _g_ _∪_ U _e_


_̸_


_̸_


_̸_


_̸_


_̸_






_̸_


_̸_


_̸_


_̸_


_̸_



_∥_ _**η**_ 0 _∥_ 1 + �
 ( _k,l_ ) _∈_ U _g_


_̸_


_̸_


_̸_


_̸_


_̸_



 (17)


_̸_


_̸_


_̸_


_̸_


_̸_




[ _**ρ**_ _k,nk_ ] _k∈_ K _g,_
� _ηk,l_ � U _∪_ U


_̸_


_̸_


_̸_


_̸_


_̸_



**s. t.** : (2) _,_ (3) _,_ (4)


_**η**_ 0 _≥_ 0 (18)

**g** _k_ _[′]_ _[−]_ **[g]** _[ ≤]_ _**[γ]**_ **[g]** [ˆ] _∀k ∈K_ _g_ (19)

_g_ ~~_i_~~ _≤_ _g_ _k,i_ _[′]_ _[≤]_ ~~_[g]_~~ _i_ _[,]_ _∀i_ _∈G, ∀k ∈K_ _g_ _, i_ = _̸_ _k_ (20)

_g_ _k,k_ _[′]_ [= 0] _∀k ∈K_ _g_ (21)

**1** _[⊤]_ **g** _k_ _[′]_ [=] **[ 1]** _[⊤]_ **[d]** _∀k ∈K_ _g_ (22)

_|g_ _k,i_ _[′]_ _[−][g]_ _[i]_ _[−][n]_ _[k]_ _[γ]_ _[i]_ _[g]_ [ˆ] _[i]_ _[|≤]_ _[g]_ [ˆ] _[i]_ _[ρ]_ _[k,i]_ _∀i_ _∈G, ∀k ∈_ K _g_ _, i_ = _̸_ _k_ (23)

_g_ _i_ + _n_ _k_ _γ_ _i_ _g_ ˆ _i_ _≥_ _g_ ˆ _i_ _ρ_ _k,i_ + _g_ _i_ _∀i_ _∈G, ∀k ∈_ K _g_ _, i_ = _̸_ _k_ (24)

_g_ _k,i_ _[′]_ _[≥]_ _[g]_ [ˆ] _[i]_ _[ρ]_ _[k,i]_ [+] _[g]_ ~~_i_~~ _∀i_ _∈G, ∀k ∈_ K _g_ _, i_ = _̸_ _k_ (25)

_f_ ~~_l_~~ _−η_ _k,l_ _≤_ _f_ _k,l_ _≤_ _f_ _l_ + _η_ _k,l_ _∀_ ( _k, l_ ) _∈_ U _g_ (26)

_f_ ~~_l_~~ _−η_ _k,l_ _≤_ _f_ _l_ +( _f_ _k_ **L** _k_ ) _l_ _≤_ _f_ _l_ + _η_ _k,l_ _∀_ ( _k, l_ ) _∈_ U _e_ (27)

_η_ _k,l_ _≥_ 0 _∀_ ( _k, l_ ) _∈_ U _g_ _∪_ U _e_ (28)


in [23] to address this computational challenge. This section
reviews the CCGA briefly as the PDL-SCOPF framework proposed in this paper borrows and adapts one of its contributions.

The CCGA is summarized in Algorithm (1). It iteratively
solves a master problem that contains a subset of the contingencies. Additional contingencies with violated constraints

_̸_

are then identified and added to the master problem after
each iteration. Model 2 presents the master problem for the
CCGA, which contains three types of constraints. The first
set of constraints considers those for the base case that are

the same as in the extensive problem 1. The second set
(Constraint (19)) is concerned with the _provisional_ generation
dispatch **g** _k_ _[′]_ [for each generator contingency] _[ k]_ [ computed by]
the master problem. Constraint (19) ensures that **g** _k_ _[′]_ [always]
remains within the limits of the APR. The third set of

constraints considers a subset of generator contingencies K _g_,
for which binary variables _ρ_ _k,i_ and global signal variable
_n_ _k_ are defined. It expresses the proportional response on the
provisional dispatch, as well as sets of likely active constraints
for the generator (U _g_ ) and line contingencies (U _e_ ) respectively.
After solving the master problem 2, the CCGA computes
the candidate generation dispatch **g** _k_ [(] _[j]_ [)] under generator contingencies _k_ (where _j_ is the iteration number). Note that the
provisional generation dispatch **g** _k_ _[′]_ [is corrected by performing]
the _binary search_, introduced in [3], on _n_ _k_ for each generator
contingency _k_ . This binary search procedure is designed to
satisfy the power balance constraints. Also, note that the
purpose of the provisional generation dispatch **g** _k_ _[′]_ [in the]
master problem is to guarantee the existence of the candidate
generation dispatches that satisfy the power balance constraints
(as found by the binary search). Using **g** [(] _[j]_ [)] and **g** _k_ [(] _[j]_ [)] [, the]
CCGA then calculates the thermal limit violations for all

generator and line contingencies. If no violations are detected,
it means that the current master problem has produced an
optimal solution, and CCGA stops. Otherwise, U _g_ and U _e_ are
updated by adding constraints with violations greater than the
threshold _β_ . CCGA converges in a finite number of iterations


x
Input







y


λ



Fig. 1: Overview of the Self-Supervised Primal-Dual Learning.


since it adds the generator contingencies with the violated
thermal limit violations in each iteration.


The PDL framework for SCOPF proposed in this paper
adapts the binary search of the CCGA in its end-to-end
pipeline. The adaptation is presented in detail later in the paper.


IV. P RIMAL -D UAL L EARNING


This section reviews Primal-Dual Learning (PDL) [25], the
machine learning framework used to approximate the SCOPF.


_A. The Augmented Lagrangian Method_


Consider the following optimization problem


min **y** _[f]_ **[x]** [(] **[y]** [)][ subject to] **[ h]** **[x]** [(] **[y]** [) = 0] _[.]_ (29)


where **x** represents instance parameters that determine the
objective function _f_ **x** and the equality constraint **h** **x** . The
_penalty method_ addresses this problem by solving a sequence
of unconstrained optimization problems of the form


_f_ **x** ( **y** ) + _ρ_ **1** _[⊤]_ _ν_ ( **h** **x** ( **y** )) _,_ (30)


where _ν_ ( _·_ ) is the element-wise violation penalty function, i.e.,
_ν_ ( _x_ ) = _x_ [2], and _ρ_ is the penalty coefficient. The _Augmented_
_Lagrangian Method_ (ALM) [32]–[35] extends the penalty
method and solves unconstrained optimization problems of the
form
_f_ **x** ( **y** )+ _**λ**_ _[T]_ **h** **x** ( **y** ) + _[ρ]_ (31)

2 **[1]** _[⊤]_ _[ν]_ [ (] **[h]** **[x]** [(] **[y]** [))]


where _**λ**_ are the Lagrangian multiplier approximations. These
multipliers are updated using the rule


_**λ**_ _←_ _**λ**_ + _ρ_ **h** **x** ( **y** ) _._ (32)


_B. Self-Supervised Primal-Dual Learning_


PDL is a self-supervised method for training deep neural
networks for learning constrained optimization problems of
the form (29). PDL mimics the ALM on a set of training
instances, with the aim of producing approximations to the
primal and dual optimal solutions for unseen problems from
the same distribution. An overview of PDL is shown in Figure
1. PDL is composed of a primal network and a dual network
that are trained iteratively in sequence to predict the primal
solutions and the Lagrangian multipliers of the ALM.



5


At each iteration, the _primal learning_ step updates the
parameters _θ_ of the primal network _P_ _θ_ while keeping the dual
network _D_ _ϕ_ fixed. Given the output of the frozen dual network
_**λ**_ and the penalty coefficient _ρ_, the primal learning uses the
loss function

_L_ p ( **y** _|_ _**λ**_ _, ρ_ )= _f_ **x** ( **y** )+ _**λ**_ _[T]_ **h** **x** ( **y** ) + _[ρ]_ 2 **[1]** _[⊤]_ _[ν]_ [ (] **[h]** **[x]** [(] **[y]** [))] _[,]_ (33)


which is the direct counterpart of the unconstrained optimization (31). After completion of the primal learning, PDL applies
a _dual learning_ step that updates the parameters _ϕ_ of the dual
network _D_ _ϕ_ . The dual learning training uses the loss function


_L_ d ( _**λ**_ _|_ **y** _,_ _**λ**_ _k_ _, ρ_ )= _∥_ _**λ**_ _−_ ( _**λ**_ _k_ + _ρ_ **h** **x** ( **y** )) _∥_ _,_ (34)


which is the direct counterpart of the update rule for the
Lagrangian multipliers of the ALM (32). Unlike the ALM,
the Lagrangian multipliers are the output returned by the
dual network. In order to update the Lagrangian multipliers,
PDL employs the _copied dual network D_ _ϕ_ _k_ to obtain the
Lagrangian multipliers _**λ**_ _k_ of the current outer iteration _k_ and
update the dual network _D_ _ϕ_ _k_ using the loss function (34). This
approach maintains the scalability of the learning process by
avoiding to store the dual values for all training instances.

To handle severe violations, each iteration may increase
the penalty coefficient _ρ_ . At iteration _k_, this update uses the
maximum violation _v_ _k_ defined as


_v_ _k_ = max (35)
**x** _∼D_ _[{∥]_ **[h]** **[x]** [(] **[y]** [)] _[∥]_ _[∞]_ _[}][,]_


where _D_ = _{_ **x** [(] _[i]_ [)] _}_ _[N]_ _i_ =1 [is the training dataset. The penalty]
coefficient increases when the maximum violation _v_ _k_ is greater
than a tolerance value _τ_ times the maximum violation from

the previous iteration _v_ _k−_ 1, i.e.,


_ρ ←_ min _{αρ, ρ_ max _}_ if _v_ _k_ _> τv_ _k−_ 1 _,_ (36)


where _τ ∈_ (0 _,_ 1) is the tolerance, _α>_ 1 is an update multiplier,
and _ρ_ max is an upper bound on the penalty coefficient. In other
words, this update process increases the penalty coefficient
only when the violation exceeds the specified tolerance, up to
the maximum value _ρ_ max .
The overall PDL procedure is specified in Algorithm (2),
and alternates between updating the primal and dual networks
during training. This iterative process resembles the ALM
for constrained optimization and replaces the unconstrained
optimizations and the Lagrangian updates in each outer iteration by the training of the primal and dual networks. At
each iteration, these networks approximate the solutions of the
unconstrained optimizations and the Lagrangian multipliers
for all instances in the training set. At inference time, these
networks return approximations of the primal and dual optimal
solutions for unseen problem instances.


V. P RIMAL -D UAL L EARNING FOR SCOPF


This section describes PDL-SCOPF, the framework that
applies PDL to learn large-scale SCOPFs. The primal
variables to approximate for the SCOPFS are **y** :=
**g** ˜ _, {_ **g** ˜ _k_ _,_ ˜ _n_ _k_ _,_ ˜ _ρ_ _k_ _}_ _k∈K_ _g_ _, {_ _**η**_ ˜ _k_ _}_ _{_ 0 _}∪K_ _g_ _∪K_ _e_, the objective function
_f_ **x** ( **y** ) is the original objective function (1) of Model 1, and


**Algorithm 2** Primal-Dual Learning (PDL) [25]
**Parameter** : Initial penalty coefficient _ρ_, Maximum outer
iteration _K_, Maximum inner iteration _L_, Penalty coefficient
updating multiplier _α_, Violation tolerance _τ_, Upper penalty
coefficient safeguard _ρ_ max
**Input** : Training dataset _D_
**Output** : learned primal and dual nets _P_ _θ_, _D_ _ϕ_

1: **for** _k ∈{_ 1 _, . . ., K}_ **do**
2: **for** _l ∈{_ 1 _, . . ., L}_ **do** _▷_ Primal Learning

3: Update _P_ _θ_ using _∇_ _θ_ _L_ p (See Eq. (33))
4: **end for**

5: Calculate _v_ _k_ as Eq. (35)
6: Define _D_ _ϕ_ _k_ by copying _D_ _ϕ_
7: **for** _l ∈{_ 1 _, . . ., L}_ **do** _▷_ Dual Learning

8: Update _D_ _ϕ_ using _∇_ _ϕ_ _L_ d (See Eq. (34))
9: **end for**

10: Update _ρ_ using Eq. (36)
11: **end for**

12: **return** _P_ _θ_ and _D_ _ϕ_







6


**Algorithm 3** The Binary Search Layer BSLayer( **g**, _k_ )


**Parameter** : Maximum iteration _t_

Initialize: _n_ _k_ =0 _._ 5, _n_ min =0, _n_ max =1

1: **for** _j_ = 0 _,_ 1 _, . . ., t_ **do**
2: _g_ _k,i_ [(] _[j]_ [)] _[←]_ [min] _[{][g]_ _[i]_ [+] _[n]_ _[k]_ _[γ]_ _[i]_ _[g]_ [ˆ] _[i]_ _[, ]_ ~~_[g]_~~ _[i]_ _[}]_ [,] _[ ∀][i][ ∈G]_
3: _g_ _k,k_ [(] _[j]_ [)] _[←]_ [0]
4: _e_ _k_ _←_ **1** _[⊤]_ **g** _k_ [(] _[j]_ [)] _−_ **1** _[⊤]_ **d**
5: **if** _e_ _k_ _>_ 0 **then:** _n_ max _←_ _n_ _k_
6: **else:** _n_ min _←_ _n_ _k_
7: _n_ _k_ _←_ 0 _._ 5( _n_ max + _n_ min )
8: **end for**
9: **for** _i ∈G_ **do**
10: **if** _g_ _i_ + _n_ _k_ _γ_ _i_ _g_ ˆ _i_ _>_ ~~_g_~~ _i_ **then:** _ρ_ _k,i_ _←_ 1
11: **else:** _ρ_ _k,i_ _←_ 0
12: **end for**
13: **return g** _k_ [(] _[j]_ [)] _[, n]_ _[k]_ _[,]_ _**[ ρ]**_ _[k]_


_a) The Fully Connected Layers:_ The fully connected
layer produces an approximation ˇ **g** of the nominal dispatch
that satisfies the generator bound constraints (4). This can be
achieved by applying an element-wise sigmoid function to the
last layer of the fully connected neural network [19], [25].
_b) Power Balance Repair Layer for Base Case:_ The
power balance repair layer is borrowed from [36] and guarantees that the primal network generates a nominal dispatch that
satisfies the power balance constraint (2). It receives ˇ **g** as an
input and generates a new nominal dispatch ˜ **g** by applying a
proportional scaling of all generators as follows:



˜
**g** =







Nominal dispatch


Generator contingency
dispatch


Slacks



x


Input Parameter



$g !


$𝜂 !



(1 _−_ _ζ_ _[↑]_ )ˇ **g** + _ζ_ _[↑]_ ~~**g**~~ if **1** _[⊤]_ **g** ˇ _<_ **1** _[⊤]_ **d** _,_
(37)
�(1 _−_ _ζ_ _[↓]_ )ˇ **g** + _ζ_ _[↓]_ **g** otherwise,


|g"<br>Fully Connected Power Balance<br>Layer Repair Layer<br>Primal Net<br>Binary Search<br>Layer<br>eter<br>Estimating<br>Slacks|Col2|
|---|---|
|eter<br>**Primal Net**|eter<br>**Primal Net**|



where _ζ_ _[↑]_ and _ζ_ _[↓]_ are defined as,





λ Lagrange multiplier



ˇ (38)
**1** _[⊤]_ **g** _−_ **1** _[⊤]_ **g** _[.]_



Fig. 2: The Primal and Dual Networks of PDL-SCOPF.


the constraints **h** **x** ( **y** ) capture the power balance equations (5)
for the generator contingencies as explained shortly. Figure 2
provides a schematic representation of the primal and dual
networks which, given the input configuration vector **x**, estimate the primal and dual solutions for SCOPF, respectively.
There are two key innovations is the design of the primal
learning network of PDL-SCOPF: (1) the use of a repair layer
for restoring the power balance of the base case and (2) the use
of a binary search layer to estimate the generator dispatches
in the generator contingencies.


_A. The Primal Network_


The primal network estimates the nominal dispatch, the contigency dispatches, and the slacks of the thermal constraints.
As shown in Figure 2, it uses three main components: (1) a
fully connected layer that produces a first approximation **ˆg** of
the nominal dispatch; (2) a power balance feasibility layer **˜g**
that produces a second approximation of the nominal dispatch
that is guaranteed to satisfy the power balance constraints; and
(3) a binary search layer that computes an approximation **˜g** **k**
to the contingency dispatches by mimicking the binary search
of the CCGA. _These three components, and the computation_
_of the constraint slacks, constitutes a differentiable program_
_for the primal learning step that is trained end-to-end._



Similarly, when **1** _[⊤]_ **g** ˇ _≥_ **1** _[⊤]_ **d**, the power balance layer is
designed to satisfy the power balance constraint for the base
case. For more details, please refer to [36]. This power balance
layer is differentiable almost everywhere and can thus be
naturally integrated into the whole training process.

_c) The Binary Search Layer for Generator Contingen-_
_cies:_ Estimating the dispatches for all _N −_ 1 generator contingencies presents a computational challenge, as the number
of binary variables grows quadratically with the number of
generator contingencies. The learning problem becomes impractical when dealing with industry-sized power grids. To
address this challenge, the primal network only predicts the
nominal dispatch and uses a binary search layer, inspired by
the CCGA algorithm, to compute the generator dispatches
under all contingencies. The _binary search layer_ is an adaption



_ζ_ _[↑]_ = **[1]** _[⊤]_ **[d]** _[ −]_ **[1]** _[⊤]_ **[g]** ˇ [ˇ]




**[1]** **1** _[⊤][⊤]_ **[d]** ~~**g**~~ _[ −]_ _−_ **1** **[1]** _[⊤][⊤]_ **g** **[g]** ˇ [ˇ] _[,]_ _ζ_ _[↓]_ = **[1]** **1** _[⊤][⊤]_ **[g]** **g** [ˇ] ˇ _[ −]_ _−_ **[1]** **1** _[⊤][⊤]_ **[d]** **g**



In the case where the total generation is smaller than the
total loads, i.e., **1** _[⊤]_ **g** ˇ _<_ **1** _[⊤]_ **d**, the coefficient _ζ_ _[↑]_ proportionally
increases ˇ **g** so that the power balance for the base case is
satisfied, i.e., **1** _[⊤]_ **g** ˇ = **1** _[⊤]_ **d** . The direct output from the fully
connected layer ˇ **g** is corrected to ˜ **g**, that is


**1** _[⊤]_ **g** ˜ = (1 _−_ _ζ_ _[↑]_ ) **1** _[⊤]_ **g** ˇ + **1** _[⊤]_ ~~**g**~~



= **1** _[⊤]_ **g** ˇ + _ζ_ _[↑]_ ( **1** _[⊤]_ ~~**g**~~ _−_ **1** _[⊤]_ **g** ˇ)

= **1** _[⊤]_ **g** ˇ + **1** _[⊤]_ **d** _−_ **1** _[⊤]_ **g** ˇ


= **1** _[⊤]_ **d** _._



(39)


of its CCGA counterpart and is described in Algorithm 3.
It estimates the dispatches under the generator contingencies
( **g** _k_ ) and the APR-related variables ( _n_ _k_ _,_ _**ρ**_ _k_ ) from the nominal
dispatches. The algorithm performs a binary search on the
global signal _n_ _k_ in order to try to find contingency dispatches
that satisfy the power balance constraint. Contrary to its
CCGA counterpart, Algorithm 3 may not always satisfy the
power balance constraint in the contingencies because ˜ **g** _k_
does not consider constraint (19), which provides such a
guarantee. In the experiment, it is observed that the provisional
constraint (19) is violated in some cases in the early stage of
the training, but it is satisfied when the training is complete.

Algorithm 3 is described for a single training instance but
it can be easily adapted to simultaneously compute these
solutions for all instances and all generator contingencies in
a minibatch. Note also that while the forward process of
the binary search layer is conducted using Algorithm 3, the
backpropagation of it to compute the first derivatives of the
contingency dispatches with respect to the base case dispatches
is obtainable through the expression in terms of ˜ **g** and _n_ _k_
(see Eq. (16)) which is efficient and differentiable almost
everywhere.
_d) Retrieving the Slack Estimates for Thermal Limits:_
Once the generation dispatches for the base case and generator
contingencies are estimated, it is possible to calculate the slack
variables for the base case, the generator contingencies (6),
and the line contingencies (12). For the base case, the slack
variables can be estimated by


˜
_**η**_ 0 = max _{_ 0 _,_ **f** _−_ **f** _,_ **f** _−_ **f** _}._ (40)


The slack variables associated with generator contingency _k ∈_
_K_ _g_ are given by


˜
_**η**_ _k_ = max _{_ 0 _,_ **K** ( **d** _−_ **Bg** _k_ ) _−_ **f** _,_ **f** _−_ **K** ( **d** _−_ **Bg** _k_ ) _}._ (41)


Element _l_ of the slack variables associated with the line

contingency _k ∈K_ _e_ is given by


˜
_**η**_ _k_ = max _{_ 0 _,_ **f** + _f_ _k_ **L** _k_ _−_ **f** _,_ **f** _−_ **f** _−f_ _k_ **L** _k_ _},_ (42)


where _l ̸_ = _k_ . If _l_ = _k_, _η_ _k,k_ _[e]_ [= 0][.]
_e) The Relaxed Constraints:_ The modeling of the original SCOPF problem (1) offers the advantageous property that
several constraints are implicitly satisfied. As previously described, the power balance constraint for the nominal case (2)
and the generation dispatch bound (4) are fulfilled through the
power balance repair layer. Additionally, by using the binary
search layer (Algorithm (3)), all the APR-related constraints
for generator contingencies (5, 7–11, 14, 15) are satisfied.
Furthermore, the lower bounds for the slack variables (13)
are also satisfied through (40), (41), and (42). Note that the
thermal limit constraints (3, 6, and 12) are soft constraints.
Hence, only the power balance constraints in the generator
contingencies may be violated. It is precisely those constraints
that are captured in the primal loss function of PDL-SCOPF,
i.e.,

**h** **x** ( **y** ) _k_ = **1** _[⊤]_ **˜g** _k_ _−_ **1** _[⊤]_ **d** ( _k ∈K_ _g_ ) _._



7


Test Case _|N|_ _|G|_ _|L|_ _|E|_ _|K_ _g_ _|_ _|K_ _e_ _|_ dim( **x** )


300_ieee 300 69 201 411 57 322 339
1354_peg 1354 260 673 1991 193 1430 1193
1888_rte 1888 290 1000 2531 290 1567 1580
3022_goc 3022 327 1574 4135 327 3180 2228
4917_goc 4917 567 2619 6726 567 5066 3753
6515_rte 6515 684 3673 9037 657 6474 5041


TABLE I: Specifications of the SCOPF Test Cases.


_B. The Dual Network_


Since all the other constraints are satisfied, the dual network
produces optimal dual estimates _**λ**_ = _D_ _ϕ_ ( **x** ) for the generator
contingency power balance constraints. Experiments were run
to evaluate whether adding the output of the primal network
to the input of the dual network would yield performance
improvements. The results were inconclusive, so the PDLSCOPF follows the schema from Section IV.


VI. E XPERIMENTS


_A. The Experimental Settings_


_a) Test Cases:_ The effectiveness of PDL-SCOPF is assessed on six specific cases from the Power Grid Library
(PGLIB) [37] given in Table I. Note that the sizes of _K_ _g_
and _K_ _e_ may differ from the actual number of generators
and transmission lines. Indeed, the experiments exclude contingency scenarios where the generator capacity is zero or
the lower limit _g_ is negative, which indicate the possible
presence of dispatchable loads. Regarding line contingencies,
the experiments exclude transmission lines that disconnect the
network completely. These lines can be identified using the
LODF matrix **L** [28].
_b) Data Perturbation:_ The data set includes instances
obtained by perturbing the load demands, the cost coefficients, and the upper bounds of the generation dispatch, i.e.,
_x_ := _{_ **d** _,_ **c** _,_ ~~**g**~~ ~~_}_~~, in the PGLIB configuration. This generalizes
earlier settings [20], [24], [25], where the only load demand
**d** is perturbed. As in [20], [24], [25], the load demands were
sampled from a truncated multivariate Gaussian distribution
defined as


**d** _∼T N_ ( **d** 0 _,_ Σ _,_ (1 _−_ _µ_ ) **d** 0 _,_ (1 + _µ_ ) _µ_ **d** 0 ) _,_ (43)


where **d** 0 is the base load demands defined in the PGLib and
Σ is the covariance matrix. Element Σ _ij_ of Σ is defined as
Σ _ij_ = _ασ_ _i_ _σ_ _j_ where _σ_ _i_ and _σ_ _j_ are proportional to _d_ _i_ and _d_ _j_
(i.e., scaled by the Z-score representing the 95th percentile of
a normal Gaussian with an unit standard deviation), and the
correlation coefficient _α_ is 1 when _i_ = _j_ and 0 _._ 5 otherwise.
_µ_ is set to 0 _._ 5, meaning that the load demands were set to be
perturbed by _±_ 50% at most. To perturb the cost coefficients
and the dispatch upper bounds, the experiments use the
base values already provided in PGLIB. These base values
are then adjusted by multiplying by factors specific to each
instance. Specifically, per instance, two independent factors
were sampled from the multivariate Gaussian distributions
_N_ ( **1** _,_ Σ), where Σ uses a correlation coefficient _α_ of 0 _._ 8.
The cost coefficients **c** are safeguarded to be nonnegative.


The dispatch upper bounds are truncated to be greater than
its lower bounds as ~~**g**~~ _←_ max ~~_{_~~ ~~**g**~~ ~~_,_~~ **g** + 0 _._ 01ˆ **g** _}_ . The dimension
of the input parameter dim( **x** ) is the same as 2 _|G|_ + _|L|_ .
_c) Data Generation:_ To evaluate PDF-SCOPF, ground
truths of 1,000 instances are sampled from the specified
distributions. These instances were solved using the CCGA
using the parameter settings suggested in [3]. The penalty
coefficient for slacks _M_ _η_ is set to 1500 as in [36]. The code is
implemented based on Pyomo [38], and solved using Gurobi
10.0 [39] on 24 physical cores of a machine with Intel Xeon
2.7GHz with 256GB RAM. Table II highlights the size of
the extensive SCOPF formulation (1): it gives the numbers of
variables and constraints before and after applying the Presolve
(with the default setting) in Gurobi. Presolve cannot even
complete due to out of memory for the larger test cases. For the
3022_goc, there are about 14.6 million continuous variables
and 106,600 binary variables. In the largest case 6515_rte,
there are about 64.9 million continuous variables, 449,400
binary variables, and 130.7 million constraints. Table III
presents the computing times and the number of iterations of
the CCGA. Observe the challenging 6515_rte case, which
requires at least 4648 seconds to solve.
_d) Baselines:_ PDL-SCOPF is evaluated against three
baselines to assess its performance. _It is important to stress_
_that all baselines use the same primal neural architecture,_
_including the repair layer for the power balance in the base_
_case._ Only PDL-SCOPF also uses a dual network. The first
baseline, denoted as _Penalty_, is a self-supervised framework
that uses the penalty function (Eq. 30) as a loss function. The
other two baselines are supervised learning (SL) frameworks.
The first supervised learning framework, _Na¨ıve_, uses a loss
function that minimizes the distance between the base case

generation dispatch estimates and the ground truth, defined

˜
as _∥_ **g** _−_ **g** _[∗]_ _∥_ 2 . The second supervised learning framework
is inspired by the _Lagrangian Duality_ (LD) approach from

[20]: it combines the Na¨ıve loss function with a penalty for
constraint violations, i.e., loss function


˜
_∥_ **g** _−_ **g** _[∗]_ _∥_ 2 + _ρ_ **1** _[⊤]_ _ν_ ( **h** **x** ( **y** )) _._ (44)


Contrary to [20], the penalty coefficient _ρ_ is not updated
and is set to 1e3, which was chosen after hyperparameter
tuning. Note that to test supervised frameworks, a significant
number of training instances need to be accumulated. For
the supervised learning baselines, 10,000 training instances
for the six test cases were accumulated. The results will

show that solving these SCOPF problems is time-consuming,
making it prohibitive for large-scale cases. However, it was
felt important to compare the supervised and self-supervised
frameworks and demonstrate that PDL-SCOPF can achieve a

comparable or better performance without the need to solve
training instances.
_e) Architectural Details:_ Both the primal and dual networks consist of four fully-connected layers, each followed by
Rectified Linear Unit (ReLU) activations. Layer normalization

[40] is applied before the fully-connected layers for the primal
network only. The number of hidden nodes in each fullyconnected layer is proportional to the dimension of the input
parameter, and is set to be 1 _._ 5 dim( **x** ).



8


Before After

Test Case #CV #BV #Cnst #CV #BV #Cnst


300_ieee 160.2k 3.2k 327.9k 44.5k 3.2k 82.0k
1354_peg 3.3m 50.0k 6.7m 642.3k 50.0k 642.3k
1888_rte 4.8m 83.8k 9.7m 503.4k 83.8k 649.7k
3022_goc 14.6m 106.6k 29.4m  -  -  4917_goc 38.2m 321.5k 77.1m  -  -  6515_rte 64.9m 449.4k 130.7m  -  -  

TABLE II: The Number of Binary and Continuous Variables
Denoted as #BV and #CV and Constraints (#Cnst) in Extensive
SCOPF Problem Before and After Presolve. ‘ _−_ ’ indicates that
Presolve runs of out of memory. _k_ and _m_ signify 10 [3] and 10 [6] .


Solving Time (s) #Iteration


Test Case min mean max min mean max


300_ieee 7.72 32.06 99.10 4 5.06 8
1354_peg 42.66 97.10 1125.91 3 4.79 7
1888_rte 171.92 654.39 2565.6 3 4.93 6
3022_goc 628.67 6932.20 15844.52 8 10.71 17
4917_goc 3720.33 8035.86 15316.95 7 10.98 16
6515_rte 4648.31 9560.78 92430.47 4 6.65 10


TABLE III: Elapsed Time and Iterations for Solving SCOPF
Test Instances Using CCGA.


_f) Training Setting:_ The training uses a mini-batch size
of 8 and a maximum of 1,000 epochs. The models are trained
using the Adam optimizer [41] with a learning rate of 1e-4,
which is reduced by a factor of 0.1 at 90% of the total
iterations. For PDL-SCOPF, the number of outer iterations ( _K_ )
is set to 20, and the maximum number of inner iterations ( _L_ )
is set to 2,000 for both the primal and dual networks, resulting
in a total of 80 _,_ 000 iterations. This same iteration count
is applied to the other baselines. The implementation uses
PyTorch, and all models were trained on a machine equipped
with a NVIDIA Tesla V100 GPU and an Intel Xeon 2.7GHz

CPU. Averaged performance results based on five independent
training processes with different seeds are reported.
_g) Parameter Settings:_ The PDL-SCOPF parameters are
configured following the recommendations in [25]. A minor
adjustment relates to updating the dual net. When the penalty
coefficient becomes too high, updating the dual net can be
unstable due to the substantial gap between the desired dual
estimates and the current estimates in Eq. (34). To address
this, the penalty coefficient used in the loss function (Eq. 34)
is fixed at 1e-1 _,_ regardless of how the penalty coefficient is
updated (Eq. 36). The initial penalty coefficient used in the
primal loss function (Eq. 33) is set to 0 _._ 1 and may be increased
up to _ρ_ max = 1e8. For updating the penalty coefficient, _τ_ and
_α_ are set to 0 _._ 9 and 2 _._ 0, respectively. To balance the objective
function value and the constraint violations in the primal loss
function (Eq. 33), the objective function is divided by 1e5.


_B. Numerical Results_


The numerical results compare PDL-SCOPF with the
ground truth and the baselines. First, to check whether the
constraints are satisfied by the optimal solution estimates


9



Test Case



Na¨ıve LD Penalty PDL-SCOPF
(SL) (SL) (SSL) (SSL)



Test Case



Na¨ıve LD Penalty PDL-SCOPF
(SL) (SL) (SSL) (SSL)



300_ieee 0.008 0.001 0.003 0.000
1354_peg 0.022 0.003 0.000 0.000
1888_rte 0.044 0.003 0.000 0.000
3022_goc 0.088 0.006 0.000 0.000
4917_goc 0.067 0.002 0.000 0.000
6515_rte 0.001 0.000 0.000 0.000


TABLE IV: Maximum Violations on Power Balance Con
straints for Generator Contingency (in p.u.).



300_ieee 1.021 **0.908** 3.867 2.805
1354_peg 13.447 2.700 2.533 **0.856**
1888_rte 22.115 2.436 4.969 **1.960**
3022_goc 159.116 8.008 1.312 **0.983**
4917_goc 47.212 2.096 0.454 **0.210**
6515_rte 2.419 1.292 2.069 **0.815**


TABLE VI: Mean Optimality Gap (%) (best values in bold).





Test Case



Na¨ıve LD Penalty PDL-SCOPF
(SL) (SL) (SSL) (SSL)



300_ieee 5(8.77%) 2(3.51%) 2(3.51%) 0(0.00%)
1354_peg 14(7.25%) 4(2.07%) 0(0.00%) 0(0.00%)
1888_rte 31(10.68%) 7(2.41%) 0(0.00%) 0(0.00%)
3022_goc 57(17.43%) 13(3.98%) 0(0.00%) 0(0.00%)
4917_goc 42(7.41%) 12(2.12%) 0(0.00%) 0(0.00%)
6515_rte 2(0.30%) 0(0.00%) 0(0.00%) 0(0.00%)


TABLE V: The Number of Generator Contingencies with Violated Power Balance Constraints (Percentages in Parenthesis).


from ML-based methods, Tables IV–V show the violations
of the hard constraints, that is, the power balance constraint
for generator contingencies (5). Table IV reports the averaged maximum violation of the generator contingency power
balance constraints on testing instances. PDL-SCOPF and
the SSL penalty method have negligible violations of the
power balance constraints on all instances. In contrast, the
supervised methods exhibit violations that can be significant,
especially for the Na¨ıve SL baseline. Table V shows the
corresponding number of generator contingencies where the
power balance constraint is violated and its percentages over
the whole number of generator contingencies in parenthesis.
The violation are calculated with a tolerance of 1e-4. This table

also underscores PDL-SCOPF satisfies the hard constraints,
which was a primary goal of the proposed training procedure.

Second, Table VI reports the mean optimality gap in percentage, providing a comparison between the CCGA algorithm
and its learning counterparts. The results show that PDLSCOPF is almost always the strongest method with optimality
gaps often below 1%. It is only dominated on the IEEE 300bus system by LD but this result must be treated carefully
since LD violates some of the power balance constraints.
Note that PDL-SCOPF significantly dominates the Penalty
(SSL) method. Also, for the thermal limits for the base
case and contingencies, it is desirable to have smaller slack
variable values. For instance of 1354_pegase, Figure 3
represents the the sum of the slack values for the base case,
the 193 generation contingencies, and the 1430 transmission
contingencies. It only shows the slack variable values for the
transmission lines in which only positive values are observed.
The transmission lines are sorted by the total slack values for
Na¨ıve SL baseline in ascending order. PDL-SCOPF exhibits
the smallest slack values when compared with the baseline
and ground truth (Gurobi). Note that positive slack values
are quite sparsely observed only on a few transmission lines



|0.012 Naïve(SL)<br>LD(SL)<br>0.010 Penalty(SSL)<br>PDL-SCOPF(SSL)<br>0.008<br>Gurobi<br>0.006<br>0.004<br>0.002<br>0.000<br>0 10 20 30 40<br>Transmission Line|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
|0<br>10<br>20<br>30<br>40<br>Transmission Line<br>0.000<br>0.002<br>0.004<br>0.006<br>0.008<br>0.010<br>0.012<br><br>~~Naïve(SL)~~<br>LD(SL)<br>~~Penalty(SSL)~~<br>PDL~~-~~SCOPF(SSL)<br>Gurobi||||||~~Naïve~~<br>LD(SL<br>~~Penal~~|~~(SL)~~<br>)<br>~~y(SSL)~~||
|0<br>10<br>20<br>30<br>40<br>Transmission Line<br>0.000<br>0.002<br>0.004<br>0.006<br>0.008<br>0.010<br>0.012<br><br>~~Naïve(SL)~~<br>LD(SL)<br>~~Penalty(SSL)~~<br>PDL~~-~~SCOPF(SSL)<br>Gurobi||||||PDL~~-~~S<br>Gurob|COPF(SSL)<br>i|COPF(SSL)<br>i|
|0<br>10<br>20<br>30<br>40<br>Transmission Line<br>0.000<br>0.002<br>0.004<br>0.006<br>0.008<br>0.010<br>0.012<br><br>~~Naïve(SL)~~<br>LD(SL)<br>~~Penalty(SSL)~~<br>PDL~~-~~SCOPF(SSL)<br>Gurobi|||||||||
|0<br>10<br>20<br>30<br>40<br>Transmission Line<br>0.000<br>0.002<br>0.004<br>0.006<br>0.008<br>0.010<br>0.012<br><br>~~Naïve(SL)~~<br>LD(SL)<br>~~Penalty(SSL)~~<br>PDL~~-~~SCOPF(SSL)<br>Gurobi|||||||||
|0<br>10<br>20<br>30<br>40<br>Transmission Line<br>0.000<br>0.002<br>0.004<br>0.006<br>0.008<br>0.010<br>0.012<br><br>~~Naïve(SL)~~<br>LD(SL)<br>~~Penalty(SSL)~~<br>PDL~~-~~SCOPF(SSL)<br>Gurobi|||||||||
|0<br>10<br>20<br>30<br>40<br>Transmission Line<br>0.000<br>0.002<br>0.004<br>0.006<br>0.008<br>0.010<br>0.012<br><br>~~Naïve(SL)~~<br>LD(SL)<br>~~Penalty(SSL)~~<br>PDL~~-~~SCOPF(SSL)<br>Gurobi|||||||||
|0<br>10<br>20<br>30<br>40<br>Transmission Line<br>0.000<br>0.002<br>0.004<br>0.006<br>0.008<br>0.010<br>0.012<br><br>~~Naïve(SL)~~<br>LD(SL)<br>~~Penalty(SSL)~~<br>PDL~~-~~SCOPF(SSL)<br>Gurobi|||||||||
|0<br>10<br>20<br>30<br>40<br>Transmission Line<br>0.000<br>0.002<br>0.004<br>0.006<br>0.008<br>0.010<br>0.012<br><br>~~Naïve(SL)~~<br>LD(SL)<br>~~Penalty(SSL)~~<br>PDL~~-~~SCOPF(SSL)<br>Gurobi|||||||||


Fig. 3: Averaged Total Power Balance Constraint Slack Values
for Each Transmission Line in 1354_pegase.


Test Case Training Sampling


300_ieee 26min 89hr 3min
1354_peg 36min 269hr 43min
1888_rte 43min 1817hr 45min
3022_goc 1hr 7min 19256hr 7min
4917_goc 1hr 59min 22321hr 50min
6515_rte 2hr 54min 26557hr 43min


TABLE VII: Averaged Training Time (in GPU) for PDLSCOPF and Accumulated CPU Time to Prepare the Supervised
Training Dataset.


PDL-SCOPF Inference Time (ms)
Speedup
Test Case 8@GPU 1@GPU 8@CPU 1@CPU


300_ieee 5.163 5.117 12.392 3.527 9088.23 _×_
1354_peg 6.917 5.146 290.376 25.745 3771.60 _×_
1888_rte 7.956 5.145 455.846 46.876 13960.02 _×_
3022_goc 13.486 5.823 1484.019 183.879 37699.81 _×_
4917_goc 31.775 8.043 3975.468 479.881 16745.53 _×_
6515_rte 51.9370 10.576 6767.474 823.016 11616.76 _×_


TABLE VIII: Averaged Inference Time of PDL-SCOPF with
1 Instance or 8 Instances on CPU or GPU in Milliseconds and

Averaged Speedup over Gurobi on 1 CPU.


for PDL-SCOPF, meaning that the transmission thermal limits
are satisfied on most transmission lines for the contingencies.
This result also contributes to the small optimality gaps of
PDL-SCOPF (Table VI) because non-zero slack values lead to
suboptimal solutions given their high penalties in the objective.


Table VII reports the elapsed GPU times for PDL-SCOPF
training and the accumulated CPU time for generating samples
for the supervised baselines. The time to generate solutions




offline is extremely high, even with the CCGA. The selfsupervised learning including PDL-SCOPF does not necessitate the ground truth sampling process, thus brings substantial
benefits in training time. It is remarkable that, for the largest
test case 6515_rte, the training time of 2 hours and 54
minutes is significantly smaller than the time required to
solve the single worst-case SCOPF instance. The latter takes
approximately 25 hours and 40 minutes (92430.47 seconds
equivalently, as shown in Table III) in the test dataset. This
efficiency in training can be attributed to the scalable modeling
and the self-supervised nature of PDL-SCOPF.
Table VIII reports the inference time in milliseconds of
PDL-SCOPF on CPUs and GPUs for a single instance or
a batch of 8 instances. Even for the largest test case, PDLSCOPF provides a high-quality approximation to a single
instance in about 10 milliseconds and to a batch of instances

in about 50 milliseconds. Note also the significant benefits of
using GPUs. These results show that PDL-SCOPF is 4 orders
of magnitude faster than Gurobi on these instances.

_Altogether, these results show that PDL-SCOPF, once_
_trained, provides near-optimal solutions to SCOPF in millisec-_
_onds without the need to solve any instance offline. It provides_
_operators with a new tool to inform real-time decision making._


VII. C ONCLUSION


This paper introduces a self-supervised primal-dual learning
framework PDL-SCOPF to approximate the optimal solutions
for large-scale SCOPF problems. The framework produces
near-optimal feasible solutions in just milliseconds, and is selfsupervised, precluding the need to generate optimal solutions
to the training instances. PDL-SCOPF is a primal-dual learning
method that mimics the ALM framework, alternating between
approximating unconstrained optimization problems using a
primal network and estimating the Lagrangian multipliers with
a dual networks. The primal network is trained with a loss
function that combines the original objective of the SCOPF
with constraints capturing the power balance of the generator
contingencies. It features a repair layer to ensure the feasibility
of the power balance of the nominal case, and a binary search
layer (inspired by the CCGA) that computes the contingency
dispatches from the nominal dispatch. The whole framework
is trained end to end, in the ALM style.
Experiment results conducted on a range of test cases,
featuring systems with up to approximately 6,500 generators,
demonstrated the effectiveness of the proposed methodology.
By combining self-supervised learning, primal dual learning,
and implicit feasibility and completion layers, PDL-SCOPF
produces predictions with no constraints violations and small
optimality gaps that compare dominates existing supervised
and self-supervised methods.
This research represents an important step in demonstrating the scalability of self-supervised end-to-end optimization
learning frameworks. These frameworks have the potential to
transform how to learn large-scale optimization problems. In
the context of SCOPF problems, potential applications include
corrective SCOPF [5], stochastic SCOPF [42], and chance
constraint SCOPF problems [43]. Self-supervised end-to-end



10


learning may also be an avenue for large-scale optimization
challenges in expansion planning and topology optimization.


A CKNOWLEDGMENTS


This research is partly supported by NSF under Awards
2007095 and 2112533.


R EFERENCES


[1] Y. Dvorkin, P. Henneaux, D. S. Kirschen, and H. Pandˇzi´c, “Optimizing
primary response in preventive security-constrained optimal power flow,”
_IEEE Systems Journal_, vol. 12, no. 1, pp. 414–423, 2016.

[2] W. Chen, S. Park, M. Tanneau, and P. Van Hentenryck, “Learning
optimization proxies for large-scale security-constrained economic dispatch,” _Electric Power Systems Research_, vol. 213, p. 108566, 2022.

[3] A. Velloso, P. Van Hentenryck, and E. S. Johnson, “An exact and scalable
problem decomposition for security-constrained optimal power flow,”
_Electric Power Systems Research_, vol. 195, p. 106677, 2021.

[4] F. Capitanescu, J. M. Ramos, P. Panciatici, D. Kirschen, A. M. Marcolini, L. Platbrood, and L. Wehenkel, “State-of-the-art, challenges, and
future trends in security constrained optimal power flow,” _Electric power_
_systems research_, vol. 81, no. 8, pp. 1731–1741, 2011.

[5] Q. Wang, J. D. McCalley, T. Zheng, and E. Litvinov, “Solving corrective
risk-based security-constrained optimal power flow with Lagrangian relaxation and Benders decomposition,” _International Journal of Electrical_
_Power & Energy Systems_, vol. 75, pp. 255–264, 2016.

[6] Y. Li and J. D. McCalley, “Decomposed SCOPF for improving efficiency,” _IEEE Transactions on Power Systems_, vol. 24, no. 1, 2008.

[7] A. J. Ardakani and F. Bouffard, “Identification of umbrella constraints in
DC-based security-constrained optimal power flow,” _IEEE Transactions_
_on Power Systems_, vol. 28, no. 4, pp. 3924–3934, 2013.

[8] D. Bertsimas, E. Litvinov, X. A. Sun, J. Zhao, and T. Zheng, “Adaptive robust optimization for the security constrained unit commitment
problem,” _IEEE transactions on power systems_, vol. 28, no. 1, 2012.

[9] B. Zeng and L. Zhao, “Solving two-stage robust optimization problems
using a column-and-constraint generation method,” _Operations Research_
_Letters_, vol. 41, no. 5, pp. 457–461, 2013.

[10] L. Platbrood, F. Capitanescu, C. Merckx, H. Crisciu, and L. Wehenkel,
“A generic approach for solving nonlinear-discrete security-constrained
optimal power flow problems in large-scale systems,” _IEEE Transactions_
_on Power Systems_, vol. 29, no. 3, pp. 1194–1203, 2013.

[11] F. Capitanescu, “Critical review of recent advances and further developments needed in ac optimal power flow,” _Electric Power Systems_
_Research_, vol. 136, pp. 57–68, 2016.

[12] I. Aravena, D. K. Molzahn, S. Zhang, C. G. Petra, F. E. Curtis,
S. Tu, A. W¨achter, E. Wei, E. Wong, A. Gholami _et al._, “Recent
developments in security-constrained AC optimal power flow: Overview
of challenge 1 in the ARPA-E Grid Optimization Competition,” _arXiv_
_preprint arXiv:2206.07843_, 2022.

[13] C. Coffrin and P. Van Hentenryck, “A linear-programming approximation of ac power flows,” _INFORMS Journal on Computing_, vol. 26,
no. 4, pp. 718–734, 2014.

[14] C. Coffrin, H. L. Hijazi, and P. Van Hentenryck, “The qc relaxation:
A theoretical and computational study on optimal power flow,” _IEEE_
_Transactions on Power Systems_, vol. 31, no. 4, pp. 3008–3018, 2015.

[15] A. Marano-Marcolini, F. Capitanescu, J. L. Martinez-Ramos, and L. Wehenkel, “Exploiting the use of dc scopf approximation to improve
iterative ac scopf algorithms,” _IEEE Transactions on Power Systems_,
vol. 27, no. 3, pp. 1459–1466, 2012.

[16] A. Pandey, M. R. Almassalkhi, and S. Chevalier, “Large-scale grid
optimization: the workhorse of future grid computations,” _Current_
_Sustainable/Renewable Energy Reports_, pp. 1–15, 2023.

[17] J. Kotary, F. Fioretto, P. Van Hentenryck, and B. Wilder, “Endto-end constrained optimization learning: A survey,” _arXiv preprint_
_arXiv:2103.16378_, 2021.

[18] V. Nair, S. Bartunov, F. Gimeno, I. Von Glehn, P. Lichocki, I. Lobov,
B. O’Donoghue, N. Sonnerat, C. Tjandraatmadja, P. Wang _et al._,
“Solving mixed integer programs using neural networks,” _arXiv preprint_
_arXiv:2012.13349_, 2020.

[19] X. Pan, T. Zhao, M. Chen, and S. Zhang, “DeepOPF: A deep neural
network approach for security-constrained DC optimal power flow,”
_IEEE Transactions on Power Systems_, vol. 36, no. 3, pp. 1725–1735,
2020.


11




[20] F. Fioretto, T. W. Mak, and P. Van Hentenryck, “Predicting AC optimal
power flows: Combining deep learning and Lagrangian dual methods,”
in _Proceedings of the AAAI conference on artificial intelligence_, vol. 34,
no. 01, 2020, pp. 630–637.

[21] A. S. Xavier, F. Qiu, and S. Ahmed, “Learning to solve large-scale
security-constrained unit commitment problems,” _INFORMS Journal on_
_Computing_, vol. 33, no. 2, pp. 739–756, 2021.

[22] S. Park, W. Chen, D. Han, M. Tanneau, and P. Van Hentenryck,
“Confidence-aware graph neural networks for learning reliability assessment commitments,” _IEEE Transactions on Power Systems_, 2023
(to appear).

[23] A. Velloso and P. Van Hentenryck, “Combining deep learning and optimization for preventive security-constrained DC optimal power flow,”
_IEEE Transactions on Power Systems_, vol. 36, no. 4, pp. 3618–3628,
2021.

[24] P. L. Donti, D. Rolnick, and J. Z. Kolter, “DC3: A learning method for
optimization with hard constraints,” _arXiv preprint arXiv:2104.12225_,
2021.

[25] S. Park and P. Van Hentenryck, “Self-supervised primal-dual learning
for constrained optimization,” in _Proceedings of the AAAI Conference_
_on Artificial Intelligence_, vol. 37, no. 4, 2023, pp. 4052–4060.

[26] S. Park, W. Chen, T. W. Mak, and P. Van Hentenryck, “Compact
optimization learning for AC optimal power flow,” _IEEE Transactions_
_on Power Systems_, 2023 (to appear).

[27] M. Chatzos, T. W. Mak, and P. Van Hentenryck, “Spatial network decomposition for fast and scalable AC-OPF learning,” _IEEE Transactions_
_on Power Systems_, vol. 37, no. 4, pp. 2601–2612, 2021.

[28] J. Guo, Y. Fu, Z. Li, and M. Shahidehpour, “Direct calculation of
line outage distribution factors,” _IEEE Transactions on Power Systems_,
vol. 24, no. 3, pp. 1633–1634, 2009.

[29] D. A. Tejada-Arango, P. S´anchez-Martın, and A. Ramos, “Security
constrained unit commitment using line outage distribution factors,”
_IEEE Transactions on power systems_, vol. 33, no. 1, pp. 329–337, 2017.

[30] X. Ma, H. Song, M. Hong, J. Wan, Y. Chen, and E. Zak, “The securityconstrained commitment and dispatch for midwest iso day-ahead cooptimized energy and ancillary service market,” in _2009 IEEE Power &_
_Energy Society General Meeting_ . IEEE, 2009, pp. 1–8.

[31] MISO, “Real-time energy and operating reserve market software formulations and business logic,” 2023, business Practices Manual, Energy
and Operating Reserve Markets Attachment D, BPM-002-r24.

[32] M. J. Powell, “A method for nonlinear constraints in minimization
problems,” _Optimization_, pp. 283–298, 1969.

[33] R. T. Rockafellar, “Augmented lagrange multiplier functions and duality
in nonconvex programming,” _SIAM Journal on Control_, vol. 12, no. 2,
pp. 268–285, 1974.

[34] R. Andreani, E. G. Birgin, J. M. Mart´ınez, and M. L. Schuverdt, “On
augmented lagrangian methods with general lower-level constraints,”
_SIAM Journal on Optimization_, vol. 18, no. 4, pp. 1286–1309, 2008.

[35] D. P. Bertsekas, _Constrained optimization and Lagrange multiplier_
_methods_ . Academic press, 2014.

[36] W. Chen, M. Tanneau, and P. Van Hentenryck, “End-to-end feasible optimization proxies for large-scale economic dispatch,” _IEEE Transactions_
_on Power Systems_, 2023 (to appear).

[37] S. Babaeinejadsarookolaee, A. Birchfield, R. D. Christie, C. Coffrin,
C. DeMarco, R. Diao, M. Ferris, S. Fliscounakis, S. Greene, R. Huang
_et al._, “The power grid library for benchmarking AC optimal power flow
algorithms,” _arXiv preprint arXiv:1908.02788_, 2019.

[38] W. E. Hart, C. D. Laird, J.-P. Watson, D. L. Woodruff, G. A. Hackebeil,
B. L. Nicholson, J. D. Siirola _et al._, _Pyomo-optimization modeling in_
_python_ . Springer, 2017, vol. 67.

[39] Gurobi Optimization, LLC, “Gurobi Optimizer Reference Manual,”
[2023. [Online]. Available: https://www.gurobi.com](https://www.gurobi.com)

[40] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” _arXiv_
_preprint arXiv:1607.06450_, 2016.

[41] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
_arXiv preprint arXiv:1412.6980_, 2014.

[42] H. Sharifzadeh and N. Amjady, “Stochastic security-constrained optimal
power flow incorporating preventive and corrective actions,” _Interna-_
_tional Transactions on Electrical Energy Systems_, vol. 26, no. 11, pp.
2337–2352, 2016.

[43] L. Roald, F. Oldewurtel, B. Van Parys, and G. Andersson, “Security
constrained optimal power flow with distributionally robust chance
constraints,” _arXiv preprint arXiv:1508.06061_, 2015.


